{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A betting man has the opportunity to make bets on the outcomes of a sporting event. If the team he bets on wins, he wins as many dollars as he has staked on that result; if his team loses, he loses his stake. The game ends when the betting man wins by reaching his goal of $100, or loses by running out of money. On each bet, he must decide what portion of his capital to stake, in integer numbers of dollars. \n",
    "\n",
    "The problem can be formulated as an undiscounted, episodic, finite MDP. The state is the betting man’s capital, s ∈ {1, 2, . . . , 99} and the actions are stakes, a ∈ {0, 1, . . . , min(s, 100−s)}. The reward is zero on all transitions except those on which the gambler reaches his goal, when it is +1. The state-value function then gives the probability of winning from each state. A policy is a mapping from levels of capital to stakes. The optimal policy maximizes the probability of reaching the goal.\n",
    "\n",
    "We will use value iteration method (Dynamic Programming) to solve this problem and hence assume that the agent knows the environment dynamics such as reward function and number of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# Probability of home team winning\n",
    "p = 0.4\n",
    "\n",
    "# The number of states availabe\n",
    "numStates = 100\n",
    "\n",
    "# List for storing the reward value\n",
    "reward = [0 for _ in range(101)]\n",
    "reward[100]=1 # Only final state has a reward for this problem\n",
    "\n",
    "# Small threshold value for comparing the difference\n",
    "theta = 0.00000001\n",
    "\n",
    "# List to store the value function for all states form 1 to 99\n",
    "value=[0 for _ in range(101)]\n",
    "\n",
    "# List to store the amount of bet that gives the max reward\n",
    "policy = [0 for _ in range(101)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforcement_learning():\n",
    "    delta = 1\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        \"Looping over all the states i.e the money in hand for a current episode\"\n",
    "        for i in range(1,numStates):\n",
    "            oldvalue = value[i]\n",
    "            bellmanequation(i)\n",
    "            diff = abs(oldvalue-value[i])\n",
    "            delta = max(delta,diff)\n",
    "    return value, policy        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellmanequation(num):\n",
    "    \"Initialize optimal value to be zero\"\n",
    "    optimalvalue = 0\n",
    "\n",
    "    \"The range of number of bets\"\n",
    "    for bet in range(0,min(num,100-num)+1):\n",
    "        \"Amount after winning and losing\"\n",
    "        win = num + bet\n",
    "        loss = num - bet\n",
    "        \"calculate the average of possible states for an action\"\n",
    "        \"In this case it would be home team winning or away team winning\"\n",
    "        sum = p * (reward[win] + gamma * value[win]) + (1 - p) * (reward[loss] + gamma * value[loss])\n",
    "\n",
    "        \"Choose the action that gives the max reward and update the policy and value for that\"\n",
    "        if sum > optimalvalue:\n",
    "            optimalvalue = sum\n",
    "            value[num] = sum\n",
    "            policy[num] = bet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, policy = reinforcement_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00206562, 0.00516406, 0.00922547, 0.01291015,\n",
       "       0.0173854 , 0.02306368, 0.02781411, 0.03227539, 0.03768507,\n",
       "       0.0434635 , 0.05035447, 0.05765919, 0.06523937, 0.06953528,\n",
       "       0.07443124, 0.08068847, 0.08661104, 0.09421268, 0.10314362,\n",
       "       0.10865874, 0.11596663, 0.12588617, 0.13357998, 0.14414799,\n",
       "       0.16      , 0.16309844, 0.16774609, 0.17383821, 0.17936523,\n",
       "       0.1860781 , 0.19459552, 0.20172117, 0.20841308, 0.21652761,\n",
       "       0.22519525, 0.2355317 , 0.24648879, 0.25785906, 0.26430292,\n",
       "       0.27164686, 0.2810327 , 0.28991657, 0.30131902, 0.31471544,\n",
       "       0.32298812, 0.33394994, 0.34882926, 0.36036996, 0.37622198,\n",
       "       0.4       , 0.40309844, 0.40774609, 0.41383821, 0.41936523,\n",
       "       0.4260781 , 0.43459552, 0.44172117, 0.44841308, 0.45652761,\n",
       "       0.46519525, 0.4755317 , 0.48648879, 0.49785906, 0.50430292,\n",
       "       0.51164686, 0.5210327 , 0.52991657, 0.54131902, 0.55471544,\n",
       "       0.56298812, 0.57394994, 0.58882926, 0.60036996, 0.61622198,\n",
       "       0.64      , 0.64464766, 0.65161914, 0.66075731, 0.66904785,\n",
       "       0.67911715, 0.69189327, 0.70258175, 0.71261962, 0.72479141,\n",
       "       0.73779287, 0.75329756, 0.76973319, 0.78678859, 0.79645439,\n",
       "       0.80747029, 0.82154905, 0.83487485, 0.85197853, 0.87207316,\n",
       "       0.88448217, 0.90092491, 0.92324389, 0.94055495, 0.96433297,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "value = np.array(value)\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  0,  0, 10,  0, 12, 12, 14, 10, 16,\n",
       "        0,  0,  6, 20,  0, 22,  0,  0,  0,  1,  2,  3,  0,  5,  6,  7,  8,\n",
       "        0, 10, 11,  0, 38, 11, 10,  9,  0,  7,  6,  5,  0,  3,  0,  0,  0,\n",
       "        1,  2,  3,  0,  5, 44,  7, 42,  0, 10, 39, 12, 13, 11, 10,  9,  0,\n",
       "        7,  6,  5,  0,  3,  0,  0,  0,  1,  0,  3,  4,  5,  0,  7, 17,  9,\n",
       "       10, 11,  0, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
